#+TITLE: cite:moore2011nature: The nature of computation
#+ROAM_KEY: cite:moore2011nature

* Notes
:PROPERTIES:
:Custom_ID: moore2011nature
:NOTER_DOCUMENT: %(orb-process-file-field "moore2011nature")
:AUTHOR: Moore, C. & Mertens, S.
:JOURNAL:
:DATE:
:YEAR: 2011
:DOI:
:URL:
:END:

* Struct questions
** What is it about?

** What is the goal of the text?
** What is the general argument of the text?
** What are the specific arguments of the text?
*** Preface
- The theory of computation objective is to understand why and how some problems are hard while others are easy.
- Computation stands with evolution and relavitiy as one of the main lenses to understand how the world works
-
** What are the main concepts of the text?
*** Prologue
- graph
- edge
- node
- eulerian path
- eulerian cycle

* Headings
*** Preface
*** Prologue

* Justification questions
** Why am I studying this?
This is directly related to my self-image as a professional and to almost every
idea that I have nowadays. Even though I spend most of my day programming, have
a published paper using a simulation and have ideas/projects related not only to
programs, but to algorithms and even to the theory computation [fn:algorithm] my
training on that is sorely lacking to nonexistent.

That is why I intend to use this course-book as a center of gravity for other
related study projects (such as studying discrete mathematics (cite:Hall_2000
)).
** Where this might be useful?
 I believe in the social computational trinitarianism. Therefore this is useful for all my work.
** Which project or idea that I already have will benefit from this and how?
This is part of my [[file:~/Drive/Org/Projects/focus.org::*Learn the foundations in \[\[file:20200702062139-techniques_refs.org\]\[refs\]\] .][Learn the foundations in refs .]] project.

My project with Kaique on Algorithmic institutionalism comes to mind.

Others would be: 
- [[file:20200711112400-on_the_conditions_of_manipulability_of_voting_methods.org][On the conditions of manipulability of voting methods]]
- [[file:20200522151434-social_choice_and_informational_requirement.org][Social Choice and Informational Requirement]]

* Outline

In the course there are  5 units
- 1. Easy and Hard
- 2. Algorithms and Landscapes
- 3. P versus NP
- 4. Worst-case, Natural, and Random
- 5. Computation Everywhere

They will be released weekly and require at least 5 hours of effort.

The book has 15 chapters + 1 mathematical appendix; in a total of 945 pages.

The pattern matching of course with chapters is:

Unit 1: Chapters 1,2, 4 (parts of 4, at least)

Unit 2: Chapter 3

Unit 3: Chapters 4, 5, and 6

Unit 4: Spread throughout (Chapters 5 and 10 get at it, but it's not clear-cut)

Unit 5: Chapter 7

*Chapters 1-7 should be read in a linear order.*

#+BEGIN_SRC julia :results output
#=
(I should have used regex here)
(couldnt think of  a pure way of doing)
What do I want:
-(x1,x2)
-(x2,x3)
-(x3,x4)

Also, the mean and mode of this.
=#
using Distributions
let
    pages = [16, 20, 34, 60, 114, 146, 192, 242, 320, 370, 470, 526, 582, 670, 742, 838, 930 ];

    acc = [ ];

    function getdiff(x,y)
        push!(acc,y-x )
        return(y)
    end

    reduce(getdiff, pages);

    println("Size of each chapter \n $(acc)")
    println("median and mean chapter size $(median(acc))   $(mean(acc))  ")
end
#+END_SRC

#+RESULTS:
: Size of each chapter
:  Any[4, 14, 26, 54, 32, 46, 50, 78, 50, 100, 56, 56, 88, 72, 96, 92]
: median and mean chapter size 55.0   57.125

** toc
Preface (16)
1 Prologue (20)

2 The Basics (34)

3 Insights and Algorithms (60)

4 Needles in a Haystack: the Class NP (114)

5 Who is the Hardest One of All? NP-Completeness (146)

6 The Deep Question: P vs. NP (192)

7 The Grand Unified Theory of Computation (242)

8 Memory, Paths, and Games (320)

9 Optimization and Approximation (370)

10 Randomized Algorithms (470)

11 Interaction and Pseudorandomness (526)

12 Random Walks and Rapid Mixing (582)

13 Counting, Sampling, and Statistical Physics (670)

14 When Formulas Freeze: Phase Transitions in Computation (742)

15 Quantum Computation (838)

Mathematical Tools (930)

References (964)





* Lectures


** Main argument/goal/theme

** Concepts
*** Unit  1. Easy and Hard
- information flow
- eulerian path:
  - A trail that visits every *edge* only once.
- hamiltonian path
  - A trail that visits every *node* only once
- hamiltonian cycle
  - Hamiltonian path in which the beginning and ending vertices are adjacent, next to each other.
- traceable graph
  - one that contains a hamiltonian path
- exhaustive search
- exponential search tree
- polynomial vs exponential time
- divide and conquer
- recurrence equation
- (time vs memory vs communication ) scaling with n
- algorithm scaling
- Big O notation (Big O(something) = it grows at most as fpast as something)
  - The ratio \({f \over g} \) does not tend to infinity as n grows
  -
- Big Omega \(\Omega\) notation : \(f = \Omega(g) := g = O(f) \)  "f grows at least as fast as g"
  - The ratio \({f \over g} \) does not tend to zero as n grows
- Big Theta \(f = \Theta(g)\) means they grow the same, they are in big O of each other
  - The ratio (usually) goes to a constant
- little o \(f = o(g)\) if f grows more slowly than g
  - The ratio \({f \over g} \) does tend to zero as n grows
- Polynomial = \(O(n^c)\) for some constant c
  - The constant is important. If c were a function this would not be a polynomial!!!
- Exponential = \(2^{\Omega(n^c)}\) for some c>0
  - It doesnt need to be 2. It can be 10 for example.
- Decision problem
    

*** Unit 2. Algorithms and Landscapes
- mergesort
- dynamic programming
- alignment (insert,delete, change)
- maximum independent set
- greedy algorithms
- minimum spanning tree
- travelling salesman problem
- optimization landscape
- max flow
- reduction
- dating problem
- bipartite graph
- stirling approximation
- decision tree 
** Propositions
*** Unit 1. Easy and Hard
- Computation may be seen as information flow
  - In the context of information theory information flow is the transfer of
    information from a variable x to a variable y in a given process
- How did euler solve the bridge problem?
  - Bridges became edges and locations nodes
  - The constraint is : visiting all places while not crossing a bridge more than once
  - There is something with odd and even degree (*see to understand below*)
  - Something like, if there is no node with odd degree then we cant perform an eulerian cycle
  - #+begin_quote
An undirected graph has an Eulerian trail if and only if exactly zero or two
vertices have odd degree, and all of its vertices with nonzero degree belong to
a single connected component. #+end_quote
  - #+begin_quote
A graph has an Euler circuit if and only if the degree of every vertex is even.
A graph has an Euler path if and only if there are at most two vertices with odd
degree. #+end_quote

- While for eulerian paths there is a trick for hamiltonian paths we have to do
  exhaustive search. One such algorithm is the exponential search tree.
- One of the *goals of theoretical computer science* is to be able to tell -
  distinguish - when a problem is more like eulerian paths or more like
  hamiltonian paths
- A divide and conquer solution is when we break a problem into smaller problems
  then break those problems into smaller problems until we can solve the from
  the smallest step to the medium step to the big problem. That is we break a
  task into substasks of *same structure* and solve it recursively;
- We are often interested in how things change as function of the size of the system
- Apply log to two functions tends to erase their distinctions, while applying
  exp to them tends to amplify their distinctions.
- The polynomial vs exponential distinction allows us hint whether we know a
  trick or we have to do some kind of search without caring about hardware
  details
  #+begin_quote
  A polynomial-time solution indicates that the problem is understood in a general, coarse-grained sense.

  #+end_quote
- RAM = random access memory, so it can access random locations of memory unlike
  the magnetic tape which to access the mth location has to roll the tape O(m)
  times. So in the RAM \(T\) steps require \(O(T)\) memory, while in the tape it would require \(O(T^2)\). In the end all polynomial.
- Notice that order gives insight into structure, but for example \(1.001^n\) is smaller than \(n^{100}\) for \(n \leq 1.000.000\) . Pay attention to that in real world settings. 

  
*** Unit 2. Algorithms and Landscapes
- Each strategy to be used (such as greedy, or divide and conquer) to solve a
  problem relies on the problem having some *mathematical structure*
- The recurrence equation for mergesort, number of comparisons needed for a list
  of size \(n\), is the following:
  - \(T(1) =  0 \)
  - \(T(n) = 2T({n \over 2}) + n\)
  - The actual closed form solution is \(T(n) = n * (\log_2 n)\) (try to prove it )
- FFT can be thought of something like divide and conquer: you recursively split
  the time series and then recombine it. This means that we are talking about
  the same pattern, and the order of the algorithm is the same as mergesort
  \(O(n\log{n})\)
- Divide and conquer works if we can split the problem into almost independent
  parts and then glue them recursively. 
- There are other problems which are too entangled. Therefore, we have to pay
  more attention to how we progress through the problem and how one part of the
  solution affects the other part of the solution.
- If the problem is decomposable but we have to look ahead, that is, we can
  recurse over it but it is not completely decomposable then we might need
  dynamic programming. The maximum independent set problem is such a problem.
- In Dynamic programming you have a small number of initial choices which then
  break the problem into smaller parts
- Alignment (edit distance) can be thought of as an optimization problem : what
  is the minimal number of operations we should do to turn a string into another
- The boruvka solution to the minimum spanning tree is the following:
  - start with the nodes which are closest and connect them
  - at each step add an edge to the closest node that does not form a cycle 
- The traveling salesman problem is an example of problem that using a greedy
  algorithm lead us astray
- Greedy algorithms may lead you to local instead of global optimums
- The notion of opt landscape appears in many sciences: fitness landscape,
  energy landscape, payoff landscapes (trickier, because landscapes are related)
- Some problems only have a simple one mountain, mountain fuji. While other
  problems have many peaks separated by valleys.
- When thinking in terms of landscapes we have to define what is the distance.
  What do we mean when we say that one solution is close or far to another.
  Topography \(\to\) Topology.
- The max flow problem shows that we can tweak greedy algorithms a bit and they
  will perform way better. We have to think of the *moves* we are allowed to do,
  and tweak them.
- Reducing a problem to the other is when you translate examples from one
  problem to examples of another problem
- There is a version of the dating problem in which edges have weights
- We can translate the dating problem into a case of the max flow problem. This
  is an example of reduction.
- Translations between problems is the way we have to say that one problem is at
  least as had or easy as another.
- The problem of alignment can also be reduced to the problem of finding the
  shortest path.
- The max flow and matching problem are related to linear programming (popular in economics), convex
  optimization (popular in machine learning), duality
- So, the families are:
  - Divide and conquer
  - Dynamic programming
  - Greedy
  - Linear programming
  - Convex programming
- *How do we prove we have found the best algorithm for a problem?* We have to
  prove some *lower bound* for solving the problem (Open area).
- This is the *intrinsic complexity* of a problem, which is defined as the
  complexity of the most efficient algorithm that solves it.
- One way of analyzing an algorithm, for sorting for example, is to draw its decision tree and see its height. We want an algorithm with lowest height. In the end this is an argument about *information*, which is easier to argue about.
- The computer science attitude about complexity is different from the physical one. For computer science systems are not complex, but the *questions* we ask about them are.
- The intrinsic, computational, complexity of a problem is *not* subjective. It is the running time (or memory, or any other resource) of the best *possible* algorithm for answering the question.
- Upper bounds on complexity are much easier to give than lower bounds. You only have to give an algorithm and check it works.
- P = NP is exactly related to this lower bound problem !!!
- Reduction may not give us an absolute measure of intrinsic complexity, but it gives us a relative measure: one is as easy/hard as the other if we can reduce it to the other.
-


* Footnotes

[fn:algorithm] Here I'm thinking about the distinction made by  cite:Yanofsky_2010 .
