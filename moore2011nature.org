#+TITLE: cite:moore2011nature: The nature of computation
#+ROAM_KEY: cite:moore2011nature

* Notes
:PROPERTIES:
:Custom_ID: moore2011nature
:NOTER_DOCUMENT: %(orb-process-file-field "moore2011nature")
:AUTHOR: Moore, C. & Mertens, S.
:JOURNAL:
:DATE:
:YEAR: 2011
:DOI:
:URL:
:END:

* Struct questions
** What is it about?

** What is the goal of the text?
** What is the general argument of the text?
** What are the specific arguments of the text?
*** Preface
- The theory of computation objective is to understand why and how some problems are hard while others are easy.
- Computation stands with evolution and relavitiy as one of the main lenses to understand how the world works
-
** What are the main concepts of the text?
*** Prologue
- graph
- edge
- node
- eulerian path
- eulerian cycle

* Headings
*** Preface
*** Prologue

* Justification questions
** Why am I studying this?
This is directly related to my self-image as a professional and to almost every
idea that I have nowadays. Even though I spend most of my day programming, have
a published paper using a simulation and have ideas/projects related not only to
programs, but to algorithms and even to the theory computation [fn:algorithm] my
training on that is sorely lacking to nonexistent.

That is why I intend to use this course-book as a center of gravity for other
related study projects (such as studying discrete mathematics (cite:Hall_2000
)).
** Where this might be useful?
 I believe in the social computational trinitarianism. Therefore this is useful for all my work.
** Which project or idea that I already have will benefit from this and how?
This is part of my [[file:~/Drive/Org/Projects/focus.org::*Learn the foundations in \[\[file:20200702062139-techniques_refs.org\]\[refs\]\] .][Learn the foundations in refs .]] project.

My project with Kaique on Algorithmic institutionalism comes to mind.

Others would be: 
- [[file:20200711112400-on_the_conditions_of_manipulability_of_voting_methods.org][On the conditions of manipulability of voting methods]]
- [[file:20200522151434-social_choice_and_informational_requirement.org][Social Choice and Informational Requirement]]

* Outline

In the course there are  5 units
- 1. Easy and Hard
- 2. Algorithms and Landscapes
- 3. P versus NP
- 4. Worst-case, Natural, and Random
- 5. Computation Everywhere

They will be released weekly and require at least 5 hours of effort.

The book has 15 chapters + 1 mathematical appendix; in a total of 945 pages.

The pattern matching of course with chapters is:

Unit 1: Chapters 1,2, 4 (parts of 4, at least)

Unit 2: Chapter 3

Unit 3: Chapters 4, 5, and 6

Unit 4: Spread throughout (Chapters 5 and 10 get at it, but it's not clear-cut)

Unit 5: Chapter 7

*Chapters 1-7 should be read in a linear order.*

#+BEGIN_SRC julia :results output
#=
(I should have used regex here)
(couldnt think of  a pure way of doing)
What do I want:
-(x1,x2)
-(x2,x3)
-(x3,x4)

Also, the mean and mode of this.
=#
using Distributions
let
    pages = [16, 20, 34, 60, 114, 146, 192, 242, 320, 370, 470, 526, 582, 670, 742, 838, 930 ];

    acc = [ ];

    function getdiff(x,y)
        push!(acc,y-x )
        return(y)
    end

    reduce(getdiff, pages);

    println("Size of each chapter \n $(acc)")
    println("median and mean chapter size $(median(acc))   $(mean(acc))  ")
end
#+END_SRC

#+RESULTS:
: Size of each chapter
:  Any[4, 14, 26, 54, 32, 46, 50, 78, 50, 100, 56, 56, 88, 72, 96, 92]
: median and mean chapter size 55.0   57.125

** toc
Preface (16)
1 Prologue (20)

2 The Basics (34)

3 Insights and Algorithms (60)

4 Needles in a Haystack: the Class NP (114)

5 Who is the Hardest One of All? NP-Completeness (146)

6 The Deep Question: P vs. NP (192)

7 The Grand Unified Theory of Computation (242)

8 Memory, Paths, and Games (320)

9 Optimization and Approximation (370)

10 Randomized Algorithms (470)

11 Interaction and Pseudorandomness (526)

12 Random Walks and Rapid Mixing (582)

13 Counting, Sampling, and Statistical Physics (670)

14 When Formulas Freeze: Phase Transitions in Computation (742)

15 Quantum Computation (838)

Mathematical Tools (930)

References (964)





* Lectures


** Main argument/goal/theme

** Concepts
*** Unit  1. Easy and Hard
- information flow
- eulerian path:
  - A trail that visits every *edge* only once.
- hamiltonian path
  - A trail that visits every *node* only once
- hamiltonian cycle
  - Hamiltonian path in which the beginning and ending vertices are adjacent, next to each other.
- traceable graph
  - one that contains a hamiltonian path
- exhaustive search
- exponential search tree
- polynomial vs exponential time
- divide and conquer
- recurrence equation
- (time vs memory vs communication ) scaling with n
- algorithm scaling
- Big O notation (Big O(something) = it grows at most as fpast as something)
  - The ratio \({f \over g} \) does not tend to infinity as n grows
  -
- Big Omega \(\Omega\) notation : \(f = \Omega(g) := g = O(f) \)  "f grows at least as fast as g"
  - The ratio \({f \over g} \) does not tend to zero as n grows
- Big Theta \(f = \Theta(g)\) means they grow the same, they are in big O of each other
  - The ratio (usually) goes to a constant
- little o \(f = o(g)\) if f grows more slowly than g
  - The ratio \({f \over g} \) does tend to zero as n grows
- Polynomial = \(O(n^c)\) for some constant c
  - The constant is important. If c were a function this would not be a polynomial!!!
- Exponential = \(2^{\Omega(n^c)}\) for some c>0
  - It doesnt need to be 2. It can be 10 for example.
- Decision problem
    

*** Unit 2. Algorithms and Landscapes
- mergesort
- dynamic programming
- alignment (insert,delete, change)
- maximum independent set
- greedy algorithms
- minimum spanning tree
- travelling salesman problem
- optimization landscape
- max flow
- reduction
- dating problem
- bipartite graph
- stirling approximation
- decision tree 
** Propositions
*** Unit 1. Easy and Hard
- Computation may be seen as information flow
  - In the context of information theory information flow is the transfer of
    information from a variable x to a variable y in a given process
- How did euler solve the bridge problem?
  - Bridges became edges and locations nodes
  - The constraint is : visiting all places while not crossing a bridge more than once
  - There is something with odd and even degree (*see to understand below*)
  - Something like, if there is no node with odd degree then we cant perform an eulerian cycle

  #+begin_quote
An undirected graph has an Eulerian trail if and only if exactly zero or two
vertices have odd degree, and all of its vertices with nonzero degree belong to
a single connected component.
 #+end_quote


  #+begin_quote
A graph has an Euler circuit if and only if the degree of every vertex is even.
A graph has an Euler path if and only if there are at most two vertices with odd
degree.
#+end_quote

- While for eulerian paths there is a trick for hamiltonian paths we have to do
  exhaustive search. One such algorithm is the exponential search tree.
- One of the *goals of theoretical computer science* is to be able to tell -
  distinguish - when a problem is more like eulerian paths or more like
  hamiltonian paths
- A divide and conquer solution is when we break a problem into smaller problems
  then break those problems into smaller problems until we can solve the from
  the smallest step to the medium step to the big problem. That is we break a
  task into substasks of *same structure* and solve it recursively;
- We are often interested in how things change as function of the size of the system
- Apply log to two functions tends to erase their distinctions, while applying
  exp to them tends to amplify their distinctions.
- The polynomial vs exponential distinction allows us hint whether we know a
  trick or we have to do some kind of search without caring about hardware
  details
  #+begin_quote
  A polynomial-time solution indicates that the problem is understood in a general, coarse-grained sense.

  #+end_quote
- RAM = random access memory, so it can access random locations of memory unlike
  the magnetic tape which to access the mth location has to roll the tape O(m)
  times. So in the RAM \(T\) steps require \(O(T)\) memory, while in the tape it would require \(O(T^2)\). In the end all polynomial.
- Notice that order gives insight into structure, but for example \(1.001^n\) is smaller than \(n^{100}\) for \(n \leq 1.000.000\) . Pay attention to that in real world settings. 

  
*** Unit 2. Algorithms and Landscapes
- Each strategy to be used (such as greedy, or divide and conquer) to solve a
  problem relies on the problem having some *mathematical structure*
- The recurrence equation for mergesort, number of comparisons needed for a list
  of size \(n\), is the following:
  - \(T(1) =  0 \)
  - \(T(n) = 2T({n \over 2}) + n\)
  - The actual closed form solution is \(T(n) = n * (\log_2 n)\) (try to prove it )
- FFT can be thought of something like divide and conquer: you recursively split
  the time series and then recombine it. This means that we are talking about
  the same pattern, and the order of the algorithm is the same as mergesort
  \(O(n\log{n})\)
- Divide and conquer works if we can split the problem into almost independent
  parts and then glue them recursively. 
- There are other problems which are too entangled. Therefore, we have to pay
  more attention to how we progress through the problem and how one part of the
  solution affects the other part of the solution.
- If the problem is decomposable but we have to look ahead, that is, we can
  recurse over it but it is not completely decomposable then we might need
  dynamic programming. The maximum independent set problem is such a problem.
- In Dynamic programming you have a small number of initial choices which then
  break the problem into smaller parts
- Alignment (edit distance) can be thought of as an optimization problem : what
  is the minimal number of operations we should do to turn a string into another
- The boruvka solution to the minimum spanning tree is the following:
  - start with the nodes which are closest and connect them
  - at each step add an edge to the closest node that does not form a cycle 
- The traveling salesman problem is an example of problem that using a greedy
  algorithm lead us astray
- Greedy algorithms may lead you to local instead of global optimums
- The notion of opt landscape appears in many sciences: fitness landscape,
  energy landscape, payoff landscapes (trickier, because landscapes are related)
- Some problems only have a simple one mountain, mountain fuji. While other
  problems have many peaks separated by valleys.
- When thinking in terms of landscapes we have to define what is the distance.
  What do we mean when we say that one solution is close or far to another.
  Topography \(\to\) Topology.
- The max flow problem shows that we can tweak greedy algorithms a bit and they
  will perform way better. We have to think of the *moves* we are allowed to do,
  and tweak them.
- Reducing a problem to the other is when you translate examples from one
  problem to examples of another problem
- There is a version of the dating problem in which edges have weights
- We can translate the dating problem into a case of the max flow problem. This
  is an example of reduction.
- Translations between problems is the way we have to say that one problem is at
  least as had or easy as another.
- The problem of alignment can also be reduced to the problem of finding the
  shortest path.
- The max flow and matching problem are related to linear programming (popular in economics), convex
  optimization (popular in machine learning), duality
- So, the families are:
  - Divide and conquer
  - Dynamic programming
  - Greedy
  - Linear programming
  - Convex programming
- *How do we prove we have found the best algorithm for a problem?* We have to
  prove some *lower bound* for solving the problem (Open area).
- This is the *intrinsic complexity* of a problem, which is defined as the
  complexity of the most efficient algorithm that solves it.
- One way of analyzing an algorithm, for sorting for example, is to draw its decision tree and see its height. We want an algorithm with lowest height. In the end this is an argument about *information*, which is easier to argue about.
- The computer science attitude about complexity is different from the physical one. For computer science systems are not complex, but the *questions* we ask about them are.
- The intrinsic, computational, complexity of a problem is *not* subjective. It is the running time (or memory, or any other resource) of the best *possible* algorithm for answering the question.
- Upper bounds on complexity are much easier to give than lower bounds. You only have to give an algorithm and check it works.
- P = NP is exactly related to this lower bound problem !!!
- Reduction may not give us an absolute measure of intrinsic complexity, but it gives us a relative measure: one is as easy/hard as the other if we can reduce it to the other.




*** Unit 3. P vs NP
- P vs NP is about checking vs finding solutions
- P = we can find in polynomial time
- NP = we can check in polynomial time
- Obviously NP  includes P. The great question is if it is larger than P or not. 
- The *N* is not non, but non-deterministic
- A non-deterministic computer is one that can make guesses
- Strictly speaking NP is a class of *decision problems* (y-n questions)
- A can be *reduced* to B if there is a polynomial-time algorithm that translates instances of A to instances of B, so that the decision answer stays the same
- A and B here is not *one* problem, but a *class* of problems, a type of problem. Example: is not about whether one graph has a hamiltonian path, but whether *any* graph as a hamiltonian path
- The reduction is not trying to solve the problem, but to translate from one representation to another.
- There are problems B in NP s.t. any other problem in NP can reduced to them:
  - \(\forall A \in \text{NP}, A \leq B \)
- Those Bs are  *NP-complete*. This means that B is a general problem. (circuit SAT is NP complete)
- Suppose there are such Bs. If A is hard this means that B is too. This means that to be NP-complete B must be fricking hard.
- This means that if we can find that *one* B is in P then so is *every* problem which are also in NP (since they can be reduced to B) which means that \(\text{P} = \text{NP}\)
- However, if we can prove that \(\text{N} \neq \text{NP}\), that is, if we can prove that there are some problems which cannot be solved in polynomial time,  then we also discover that B cannot be solved, since all NP problems could be reduced to it
- If we run anything in our computer if goes all way down to bits which can be seen as decision problems (yes or no)
- That is why we translate stuff to boolean circuits. Our computers in the end are simply boolean circuits!
- So translating to a boolean circuit amounts to translating your question into
  "Are there values for the inputs that makes the output true?" than construct a
  boolean circuit for our problem that does that. Obviously this does not
  *solve* the problem, it is np-hard, but we can then use heuristics for
  circuits!!
- The three sat translation is to try to translate your problem into statements
  about AND OR combinations of binary variables
- 2-SAT is in P
- Polynomial-time reductions are transitive
- We can prove a problem is NP-complete by reducing it to a known NP-complete problem
- *NAESAT* (not all equal) = given 3 variables show at least one is true and that at least one is false.
   #+begin_quote
, a NAE-3-SAT clause (% 1 , % 2 , % 3 ) forbids assignments where
all three are false or where all three are true .
#+end_quote
   
- 3-SAT \(\leq\) NAESAT
- IF we want to prove that something is NP-complete we must build a NP-complete problem in terms of it, not the other way around
- N-sat = given a formula, what values of its variables make it true
  #+begin_quote
A formula is said to be satisfiable if it can be made TRUE by assigning appropriate logical values (i.e. TRUE, FALSE) to its variables. The Boolean satisfiability problem (SAT) is, given a formula, to check whether it is satisfiable
  #+end_quote
- N-sat is reducible to 3-sat (easy, group the variables into three terms)
- A *CNF* is a formula of (or literals) connected by and. Literals are variables and their negations
- 2-sat is proved using directed graphs (p.104 of the book)
- circuit-sat is different from n-sat !!!
- 3-graph coloring is NP-complete
  - Is 3 colors enough to color a graph without neighbor nodes having the same color?
- To reduce on problem to another is like *compiling* it to the other problem. We do so through *gadgets*:
  #+begin_quote
a typical reduction consists of building two kinds of gadgets: “choice”
gadgets which represent setting a variable to one of its possible values, and “constraint” gadgets which
force two or more variables to obey a certain constraint.
  #+end_quote
- Graph 3-coloring \(\leq  \) Max Independent Set
- Given the whole tree of np-complete problems the consensus is that P \(\neq\) NP
- If P = NP anything that is easy to check is also easy to find, we only don't yet know how
- P vs NP is about the nature of mathematical truth and creativity
- More precise defintion of NP: a decision problem is in NP if whenever the answer is y there is an easy to check proof that it is so
- NP is asymmetric, it is about checking if we can say *there is* something, but not that there isn't. Existence is not the same as non-existence.
- NP is only one ladder in the hierarchy of complexity.
- There are some problems in which the solution itself is exponential *to check* (counterfactually there may be some way to compress that, but we don't know)
- He know makes a distinction between time and memory. Using tic-tac-toe he shows that one can make a tree of moves. Exploring it with depth-first will take exponential time, but saving it will only take polynomial *memory*.
- *PSPACE* are exactly that: problems we can solve in polynomial memory even if they take exponential time
- Above the infinity ladder of complexity there is the *computability* set: Turing halting problem is one such problem. 
- Exam questions:
   - Given an initial state s, what will be the state at time t? P, because you only need to simulate it.
  - Does a state s have a predecessor? NP, easy to check, hard to find.
  - On a lattice of size n, is s on a periodic orbit? PSPACE. Easy to see if you code a cellular automata.
  - On an infinite lattice, will s ever die out? Undecidable, it indeed looks like the halting problem. 
*** Unit 4 Real world problems
- computational complexity is about worst case behavior, we assume instances are given by a clever adversary
- He compares the optimistic physicists with the pessimist computer scientist. It is an attitude, a mindset.
- Augustinian vs Manichaean devil
- Linear programming is about trying to get to the top of a polytope (simplex appears here)
- If you add some *noise* the polytope gets smoother therefore easier to get to the top: *Smoothed analysis*
- Another way that real world problems may be easy is that their landscape might be bumpy but local optimums might be actually close to the global optimum. K-means for example works reasonably well in such scenario (if you are clustering).
- Selective forces might make problems easy because hardness goes against fitness
- The *threshold conjecture*
- The is a transition from solvability to unsolvability as we make the sat problem bigger. The search is the highest IN the transition, which is *crazy*
- Heavy tails are cases in which the average behavior has nothing to do with the typical behavior.
- In the SAT complexity transition we can see that a transition from a single peak to clusters of peaks. Monte Carlo algorithms wont do well in such "glassy" configuration. After clustering if one heigthens the hardness the cluster start to "freeze": variables take a fixed value and it becomes easy to get it wrong. This is when an algorithm really becomes hard, this freezing transition.
- This means that there are other phase transitions besides the transition from satisfiable to unsatisfiable. One goes from simple, to clustered, to frozen, to unsolvable
- OR is a nonlinear operation, while XOR is an affine operation
*** Unit 5 - Computation Everywhere
- What does a complex system need to possess in order to compute? What are the simplest ingredients that suffice?
- What does it mean for a function to be computable?
- Hilbert called and *effective procedure* what we know call an algorithm.
- Unfolding what is computable was one of the main projects in logic in mathematics of the early 20century
- *Partial Recursive functions* (dedekind, skolem, peter)
- Given a 0 an identity and a successor function, which we assume are computable, what are the building blocks?
  - Composition
    - from a programmer perspective this is like calling a function as a subroutine in another routine
  - *Primitive recursion*
    [[file:~/Drive/Org/imgs/primitive-recursion.png]]
  - primitive recursion can be implemented with for loops
  - notice that we know that lenght of the recursion or of the loop, this means we know it will halt
- The gist of primitive recursion is that we are defining a y+1 function in terms of its previous iteration. That is why we need a base case.
#+begin_quote
Note that when we start evaluating a function defined using primitive recursion,
we know to what depth it will recurse, since evaluating h(x, y ) takes y levels
of recursion.
#+end_quote
- *Primitive recursive functions* = (0 function, Id, Succ) with composition and primitive recursion
- \(\mu\) -recursion or minimization
  - smallest y s.t. f(x,y) = 0
  - if one adds that to primitive recursive building blocks we get partial recursive functions? why? because there may be no such y, which would make the function undefined. As a program this means the while loop which we use to implement it would run forever, never halt. That is why this is a partial function, it is not defined for some input.
  - minimization is kinda equivalent to while loops. This means that as soon as your system has a while loop in it , or minimization, it might not halt.
  - Why would we need while loops or minimization if it might not halt? because there are *total functions* which are only expressible in terms of while loops.
    - An example is the ackermann function (level of exponentiation)
        #+begin_quote
    All primitive recursive functions are total and computable, but the Ackermann function illustrates that not all total computable functions are primitive recursive.
        #+end_quote
- That is while will give us some partial functions, but we need it for some total functions
- \(\lambda\) calculus is built entirely on strings of symbols and substitution
- \(\lambda x\) means replace each x with a copy of the following string
- \(\lambda s\) of more variables are solved by iterative substitution
- In \(\lambda\) calculus, and therefore in lisp, there is no distinction between function and data. This is different from standard mathematics, in which there is some hierarchy: derivatives act on functions which act on numbers. From this we get higher order functions, and therefore functional programming.
- A program is something you can *run* which does stuff to data but a program is also data itself
- He does not mention that, but that is why lisp was so important for old ai. This self-reference is something that todays AI is still not able to grasp.
- It is possible to define \(\lambda s\) which never halt. That is, the substitution process never ends. Example? \((\lambda x.xx)(\lambda x.xx)\) . Try substituting that and you will get into a infinite recursion
- Turing machine:
  - Tape with symbols in a finite alphabet A
  - Head with a finite set of states S
  - At each step, the head reads the symbol a at its current location, writes a symbol a' and moves left or right
  - There is a special state called halt, it stops running
  - Its output is what is written on the tape
- Universal turing machines : turing machines that simulate any other turing machines.
  - This is like an OS or interpreter
- The Halting problem is about computer programs that try to answer questions about other computer programs
- It is the following problem : "Is there a computer program that predicts whether any other program eventually halts?"
- The halting problem is undecidable, a proof is to give a Halts program to another program, and it is the negation of Halts (if halts it runs forever else halt). Feeding mobius to mobius leads to a contradiction. A way out is to consider that Halts doesn't even exist in the first place.
- Rice's theorem is a generalization of the halting problem: it says that any question about a turing machine's long-term behavior is undecidable since we can reduce the halting problem to it
- grand unification = partial recursive functions, lambda calculus and turing machines are equivalent models of computation
- Church-Turing *thesis* is that any reasonable model of computation is captured by those three equivalent models
- Babbage analytical engine is actually an universal computer
- List of universal computers:
  - game of life (you use the gliders)
  - rule 110
  - wang tiles
  - some dynamical systems
  - In general: given a piecewise linear map in two dimensions and an initial point x is undecidable to say whether it will ever fall into a particular region, or if is periodic, or lies on a chaotic trajectory
- Are there undecidable but not universal systems? Really weirdly behaved dynamical system? Are they too complicated to compute, but also too weird to build a computer ?

* Flashcards
** What is information flow? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T20:54:14Z
:FC_TYPE:  normal
:ID:       8c6e5b14-cb6c-4b38-b512-27e19b894c06
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T20:54:14Z |
:END:
*** Back

#+begin_quote
In the context of information theory information flow is the transfer of information from a variable x to a variable y in a given process
#+end_quote

** What is an Eulerian trail and an eulerian cycle? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T20:56:21Z
:FC_TYPE:  normal
:ID:       620dc365-3b02-49a1-9631-f3cfadc98d70
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T20:56:21Z |
:END:
*** Back

  #+begin_quote
An undirected graph has an Eulerian trail if and only if exactly zero or two
vertices have odd degree, and all of its vertices with nonzero degree belong to
a single connected component.
 #+end_quote


  #+begin_quote
A graph has an Euler circuit if and only if the degree of every vertex is even.
A graph has an Euler path if and only if there are at most two vertices with odd
degree.
#+end_quote

** How eulerian and hamiltonian paths illustrate a moral general view of the goal of computer science? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T20:58:05Z
:FC_TYPE:  normal
:ID:       6e943cda-69cb-4974-9588-c3d8c1318ea5
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T20:58:05Z |
:END:
*** Back
While for eulerian paths there is a trick for hamiltonian paths we have to do exhaustive search. One such algorithm is the exponential search tree. One of the
*goals of theoretical computer science* is to be able to tell - distinguish -
when a problem is more like eulerian paths or more like hamiltonian paths

** What is a divide and conquer solution? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T20:59:04Z
:FC_TYPE:  normal
:ID:       a167f1de-798b-4c2b-a976-35bbbe5fa7bd
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T20:59:04Z |
:END:

*** Back
 A divide and conquer solution is when we break a problem into smaller problems
  then break those problems into smaller problems until we can solve the from
  the smallest step to the medium step to the big problem. That is we break a
  task into substasks of *same structure* and solve it recursively;
** How to reduce or amplify distinctions between the behavior of two functions ? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:00:24Z
:FC_TYPE:  normal
:ID:       ed8b5893-c8a2-4252-9442-fb903821794e
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:00:24Z |
:END:
*** Back
Apply log to two functions tends to erase their distinctions, while applying exp
to them tends to amplify their distinctions.

** Why do we care about the exponential-polynomial distinction? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:01:03Z
:FC_TYPE:  normal
:ID:       8f35f476-d901-4705-9705-dd3a8f92f284
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:01:03Z |
:END:
*** Back
The polynomial vs exponential distinction allows us hint whether we know a
  trick or we have to do some kind of search without caring about hardware
  details
  #+begin_quote
  A polynomial-time solution indicates that the problem is understood in a general, coarse-grained sense.

  #+end_quote

** How the practical setting differs from the asymptotic one? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:01:56Z
:FC_TYPE:  normal
:ID:       68b61a0d-4577-4557-9ae0-e45913b68d77
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:01:56Z |
:END:
*** Back
Maybe an algorithm which performs worse asymptotically will perform better in
your case.

Order gives insight into structure, but for example \(1.001^n\) is smaller than
\(n^{100}\) for \(n \leq 1.000.000\) . Pay attention to that in real world
settings.


** What is a hamiltonian path and cycle? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:04:26Z
:FC_TYPE:  normal
:ID:       1ec305aa-6a7b-476c-b97a-7e1ca1a9f1c6
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:04:26Z |
:END:
*** Back
-  hamiltonian path
  - A trail that visits every *node* only once
- hamiltonian cycle
  - Hamiltonian path in which the beginning and ending vertices are adjacent, next to each other.
** What are the resources in algorithmic scaling analysis? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:05:27Z
:FC_TYPE:  normal
:ID:       05023760-b5a7-4218-bb9d-3772b67fb49a
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:05:27Z |
:END:

*** Back
We are often interested in how things change as function of the size of the system. That is algorithmic scaling analysis.
(time vs memory vs communication ) scaling with n is what we tend to care about.

** What is Big O notation? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:08:16Z
:FC_TYPE:  normal
:ID:       26dfc891-93d0-4038-8924-13b15dd53886
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:08:16Z |
:END:

*** Back
(Big O(something) = it grows at most as fast as something The ratio \({f \over g} \) does not tend to infinity as n grows

** What is Big Omega notation? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:08:44Z
:FC_TYPE:  normal
:ID:       9e6787df-5b3c-40e0-ace4-f6aa05e02584
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:08:44Z |
:END:
*** Back
Big Omega \(\Omega\) notation : \(f = \Omega(g) := g = O(f) \) "f grows at least
as fast as g". The ratio \({f \over g} \) does not tend to zero as n grows.
** What is Big Theta notation? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:09:26Z
:FC_TYPE:  normal
:ID:       1c23b77c-7cb8-4853-8d76-023b26d0539d
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:09:26Z |
:END:
*** Back
 Big Theta \(f = \Theta(g)\) means they grow the same, they are in big O of each
 other. The ratio (usually) goes to a constant
** What is little o notation? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:10:14Z
:FC_TYPE:  normal
:ID:       f0706d33-cce2-4977-95f4-e5dbef0cb2fb
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:10:14Z |
:END:
*** Back
little o = \(f = o(g)\) if f grows more slowly than g. The ratio \({f \over g}
\) does tend to zero as n grows
** Spell def of \(O, \Omega, \Theta, o\) notation :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:13:05Z
:FC_TYPE:  normal
:ID:       dd17aab4-0239-47a9-8131-d07d702e6e4f
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:13:05Z |
:END:
*** Back
- Big O notation (Big O(something) = it grows at most as fpast as something)
  - The ratio \({f \over g} \) does not tend to infinity as n grows
  -
- Big Omega \(\Omega\) notation : \(f = \Omega(g) := g = O(f) \)  "f grows at least as fast as g"
  - The ratio \({f \over g} \) does not tend to zero as n grows
- Big Theta \(f = \Theta(g)\) means they grow the same, they are in big O of each other
  - The ratio (usually) goes to a constant
- little o \(f = o(g)\) if f grows more slowly than g
  - The ratio \({f \over g} \) does tend to zero as n grows

** How do we say that something is polynomial or exponential in terms of algorithmic scaling ? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:14:20Z
:FC_TYPE:  normal
:ID:       065b9c57-5c6a-4993-849e-c97d3d8e8821
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:14:20Z |
:END:
*** Back
Polynomial = \(O(n^c)\) for some constant c
  - The constant is important. If c were a function this would not be a polynomial!!!
Exponential = \(2^{\Omega(n^c)}\) for some c>0
  - It doesnt need to be 2. It can be 10 for example.

** What is the difference between divide and conquer and dynamic programming problems? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:28:54Z
:FC_TYPE:  normal
:ID:       3cb218b3-1f1e-4260-adae-cc66e5c37f25
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:28:54Z |
:END:
*** Back
In both we break a bigger problem into a smaller problem. The difference is that in dynamic programming there are *overlapping subproblems* ([[https://stackoverflow.com/questions/13538459/difference-between-divide-and-conquer-algo-and-dynamic-programming][algorithm - Difference between Divide and Conquer Algo and Dynamic Programmin...]]). So, it can be decomposed, but not completely. In divide and conquer approaches you *can* decompose it completely and glue the solutions afterwards.
** What is the maximum independent set problem? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:32:13Z
:FC_TYPE:  normal
:ID:       791ff0b2-b409-4f21-86c5-0f4de6df01fb
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:32:13Z |
:END:
*** Back
You have to pick non adjacent nodes whose sum of weights will give you the highest possible value.

** What are opt landscapes? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:35:27Z
:FC_TYPE:  normal
:ID:       a045de9b-8297-4100-a530-735cd1df2890
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    | 2.50 |   1 |     0.01 | 2020-08-19T20:56:26Z |
:END:
*** Back
We use this metaphor in many contexts. fitness landscape,
  energy landscape, payoff landscapes (trickier, because landscapes are related)
- Some problems only have a simple one mountain, mountain fuji. While other
  problems have many peaks separated by valleys.
- When thinking in terms of landscapes we have to define what is the distance.
  What do we mean when we say that one solution is close or far to another.
  Topography \(\to\) Topology.

** What is the max flow problem? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:37:43Z
:FC_TYPE:  normal
:ID:       8dc3f186-76cd-4346-a89f-915f7c147dd0
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:37:43Z |
:END:
*** Back
#+begin_quote
finding a feasible flow through a flow network that obtains the maximum possible flow rate.
#+end_quote

** What is the min cut problem? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:38:44Z
:FC_TYPE:  normal
:ID:       a0c0dcd9-ad7e-4f4e-b964-75a4cb0ce208
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:38:44Z |
:END:
*** Back

#+begin_quote
In graph theory, a minimum cut or min-cut of a graph is a cut (a partition of the vertices of a graph into two disjoint subsets) that is minimal in some sense. [[https://en.wikipedia.org/wiki/Minimum_cut][Minimum cut - Wikipedia]]
#+end_quote

** What is the edit distance problem? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:40:19Z
:FC_TYPE:  normal
:ID:       3db612fe-931d-4cf4-8de2-bcf056e9b497
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:40:19Z |
:END:
*** Back
#+begin_quote
edit distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other. [[https://en.wikipedia.org/wiki/Edit_distance][Edit distance - Wikipedia]]
#+end_quote

** What is reducing regarding problems? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:41:20Z
:FC_TYPE:  normal
:ID:       0d04d614-b3da-445e-ae02-d70db2edc6ae
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:41:20Z |
:END:

*** Back
- Reducing a problem to the other is when you translate examples from one  problem to examples of another problem
- There is a version of the dating problem in which edges have weights
- We can translate the dating problem into a case of the max flow problem. This
  is an example of reduction.
- Translations between problems is the way we have to say that one problem is at
  least as had or easy as another.
- The problem of alignment can also be reduced to the problem of finding the
  shortest path.
** What then is complexity (in cs)? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:43:37Z
:FC_TYPE:  normal
:ID:       1d249a7a-052e-48ca-a5b8-15e34657b9d0
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:43:37Z |
:END:
*** Back
The computer science attitude about complexity is different from the physical one. For computer science systems are not complex, but the *questions* we ask about them are.

The intrinsic, computational, complexity of a problem is *not* subjective. It is the running time (or memory, or any other resource) of the best *possible* algorithm for answering the question.
Reduction may not give us an absolute measure of intrinsic complexity, but it gives us a relative measure: one is as easy/hard as the other if we can reduce it to the other.

** What is intrinsic complexity? :fc:
:PROPERTIES:
:FC_CREATED: 2020-07-29T21:44:32Z
:FC_TYPE:  normal
:ID:       6849e0f8-5eb5-4615-99a1-bc463d62ae3d
:END:
:REVIEW_DATA:
| position | ease | box | interval | due                  |
|----------+------+-----+----------+----------------------|
| front    |  2.5 |   0 |        0 | 2020-07-29T21:44:32Z |
:END:
*** Back
The  complexity of the most efficient algorithm that solves it.

Upper bounds on complexity are much easier to give than lower bounds. You only have to give an algorithm and check it works.
P = NP is exactly related to this lower bound problem !!!

* Footnotes

[fn:algorithm] Here I'm thinking about the distinction made by  cite:Yanofsky_2010 .
* My insights
- What is the conneciton between social choice and 3-sat. 2-sat is in P, but 3-sat is not. The same "complexity" happens with 3 alternatives. WHat the fuck is happening here? Maybe trying to prove why 2-SAT is in P might give me some insight on why 3 alternatives social choice leads to paradoxes.


- It is interesting that the framework for reduction that he uses, gadgets, is eerily  similar to what is used by [[file:list2011logical.org][list2011logical: The logical space of democracy]].


- Another thing is that Laver argument may fail if we use smoothed analysis!

- Can we reduce every social choice problem into a 3 alternatives model? Are higher dimensional models actually easier to solve?
- Can we do like fontana and apply lambda calculus model of computation as a basis for institutional theory and therefore allow behavior to act on institutions and institutions act on behavior? Or something like that?
- Walter fontana reasons in a similar way to Poisot, and says
  #+begin_quote
The challenge of systems biology is not only experimental in kind. It also is the challenge of reasoning about facts that are rapidly evolving while remaining highly fragmented across research communities. I see a fundamental role for models as reasoning instruments in biology. Models, not databases as we know them today, will become the main vehicles for the computer-assisted storage, communication, and retrieval of biological knowledge. Computer scientists and I have joined forces with several other researchers to design a computational environment that represents biological knowledge, as it pertains to signaling, in an editable and executable fashion. This instrument will lend itself to the collaborative construction and critique of models.
  #+end_quote
